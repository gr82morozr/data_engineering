# Start from an Anaconda base image
FROM continuumio/anaconda3:latest

ARG SPARK_UID=1000
ARG SPARK_VERSION=3.5.0
ARG HADOOP_VERSION=3
ARG SCALA_VERSION=2.13
ARG KAFKA_VERSION=3.6.1


# Set environment variables

ENV SPARK_HOME=/opt/spark
ENV PATH="$SPARK_HOME/bin:$PATH"

# For root
USER root

RUN apt-get update
RUN apt-get install -y curl wget openjdk-11-jdk
RUN useradd -m spark
RUN mkdir -p $SPARK_HOME

COPY ./spark/start_master.sh /opt/spark
RUN chown -R spark:spark $SPARK_HOME
RUN chmod 777 /opt/spark/start_master.sh


# Switch to spark
USER spark

# Create a new environment and initialize it
#RUN conda create -n py310 python=3.10 && conda init bash

# Set the environment variable for the conda environment
#ENV PATH /home/spark/.conda/envs/py310/bin:$PATH
#ENV CONDA_DEFAULT_ENV py310

RUN pip install --upgrade pip
RUN pip install pyspark
RUN pip install confluent_kafka
RUN pip install jupyterlab --upgrade
RUN pip install pymongo

# Update PATH for Spark
RUN mkdir -p /home/spark/notebooks
# Download and install Spark

RUN echo https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}-scala${SCALA_VERSION}.tgz
RUN wget https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}-scala${SCALA_VERSION}.tgz -O /tmp/spark.tgz
RUN cd /tmp && tar zxvf /tmp/spark.tgz
RUN mv /tmp/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}-scala${SCALA_VERSION}/*  $SPARK_HOME
RUN wget https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_${SCALA_VERSION}/${SPARK_VERSION}/spark-sql-kafka-0-10_${SCALA_VERSION}-${SPARK_VERSION}.jar -O $SPARK_HOME/jars/spark-sql-kafka-0-10_${SCALA_VERSION}-${SPARK_VERSION}.jar
RUN wget https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/${KAFKA_VERSION}/kafka-clients-${KAFKA_VERSION}.jar -O $SPARK_HOME/jars/kafka-clients-${KAFKA_VERSION}.jar
RUN wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10_${SCALA_VERSION}/${SPARK_VERSION}/spark-streaming-kafka-0-10_${SCALA_VERSION}-${SPARK_VERSION}.jar -O $SPARK_HOME/jars/spark-streaming-kafka-0-10_${SCALA_VERSION}-${SPARK_VERSION}.jar
RUN wget https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.12.0/commons-pool2-2.12.0.jar -O $SPARK_HOME/jars/commons-pool2-2.12.0.jar
RUN wget https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_${SCALA_VERSION}/${SPARK_VERSION}/spark-token-provider-kafka-0-10_${SCALA_VERSION}-${SPARK_VERSION}.jar -O $SPARK_HOME/jars/spark-token-provider-kafka-0-10_${SCALA_VERSION}-${SPARK_VERSION}.jar
RUN rm /tmp/spark.tgz

# Update conda
# RUN conda update -n base -c defaults conda -y 
# RUN conda install -c conda-forge jupyterlab

ENV JUPYTER_TOKEN=""
ENV JUPYTER_PASSWORD=""
ENV JUPYTER_ENABLE_LAB=yes
